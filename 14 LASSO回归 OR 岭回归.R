###岭回归/LASSO回归###
 
  ##########################   概  述   ###########################
  # 建模的主要目的是：选择重要的变量而剔除无关紧要的变量
  # 多种方法可实现这一目的：如最佳子集法、目的性建模、逐步回归：回归系数的惩罚（LASSO/RIDGE（岭回归））
  # 传统建模的思路是通过最小二乘法求取参数
  # 但是，当训练数据数据集的数量小于特征变量时，模型会不稳定，此时的变量容易存在共线性
  # 存在共线性时，系数会很大；
  # 两个互为共线性的变量，其系数会很大，方差也很大且符号相反，出现临床难以解释的情况
  
  #LASSO是将正则化部分用变量参数的绝对值之和进行替代
  #岭回归则是普通的正则化项
  
  #Elastic net penalty 结合了岭回归与lasso回归，此时用CV数据集求取lambda取值
  
  ##########################  lambda变化规律   ###########################
  library(glmnet)
  x = matrix(rnorm(100*20),100,20)
  y = sample(1:2,100,replace = TRUE)
  fit = glmnet(x,y,family="binomial")
  plot(fit,xvar = 'lambda')  
  
  #结果解读：
  #最下面为lambda取值，侧面为协方差系数，最上面为特征量的个数
  #当lambda取值很小时，所有变量都存在；
  #随着lambda值增加，变量个数逐渐减少，即不断有变量的协方差系数变为0
  #当lambda值极大时，只剩下一个变量
  
  print(fit)
  #结果解读
  #df表示变量的个数；
  #%Dev表示偏离程度的百分数
  #Lambda表示其不同的取值
  
  predict(fit,type = 'response', newx = x[2:5,])
  #s0,s1代表不同的lambda值，从而代表不同的变量参数，具有不同的预测结果
  
  predict(fit,type = 'response', newx = x[2:5,],s=0.1046000)
  #可以指定特定的s值进行计算
  
  coef(fit,s=0.1046000)
  #返回拟合函数留下的变量及其系数
  
  ##########################  lambda取值   ###########################
  
  cvfit = cv.glmnet(x,y)
  plot(cvfit)
  #横坐标是lambda，纵坐标是预测值与观测值的差距，最上面为变量的个数 
  #从左至右第一条虚线表示最小值，第二条虚线表示取最小值的1个标准差之内的值
  cvfit$lambda.min
  
  cvfit$lambda.1se
  
  coef(cvfit,s='lambda.min')
  
    